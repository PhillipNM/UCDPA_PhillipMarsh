{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86234c70",
   "metadata": {},
   "source": [
    "<H1 style=\"background-color:#00008b;color:#F8F8FF;\"><center>\n",
    "    Project Report - Phillip Marsh\n",
    "    </center></H1>\n",
    "<hr>\n",
    "\n",
    "<H2 style=\"color:#00008b;\">GitHub URL</H2>\n",
    "Phillip's GutHub can be found at: <a href=\"https://github.com/PhillipNM/UCDPA_PhillipMarsh\" target=\"_blank\">Phillip's GitHub Repository</a>\n",
    "<pre style=\"font-family:arial;\">\n",
    "document should contain between 1,500 and 2,00 words\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e5044",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#00008b;\">Abstract</h2>\n",
    "(short overview of the entire project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabde8b",
   "metadata": {},
   "source": [
    "For this project I chose to review COVID data as I was somewhat familiar with the underlying data but only from creating metrics on the data.  I wanted to gain some further understanding of the situation and felt there would be a lot of data options available.  The results did not turn out as I planned but the excercise was rewarding but very challenging. Trying to cover such a large scope of skills with in python and the huge amount of imformation on tips and tricks, although many sites are not that useful and I spent hours between the DataCamp videos and onlineadvice sites.  It turns out that population density and economic prosperity of a country does not have much of an impact of a disease like COVID which I guess is why people are not panicking each flu season.  I would have loved to added some insights into the impact of masking, and lock downs but tring to join that periodic data in with this daily data was too much of a challenge for this short period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc2d3d",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#00008b;\">Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca01f0c",
   "metadata": {},
   "source": [
    "(Explain why you chose this project use case)\n",
    "\n",
    "After considering serveral ideas and researching the available dataset I decide on a dataset I am fairly familiar with from a reporting point of view (as part of the business continuity team) but that I had not done much with the other than create some metrics using Tableau.  I wondered if we could predict confidently that countries with lower population densities or high GDP per capita fared better than higher density countries or lower GDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30d4c2",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#00008b;\">Datasets</h2>\n",
    "(Provide a description of your dataset and source. Also justify why you chose this source)\n",
    "\n",
    "<h6>Deciding on the dataset</h6>\n",
    "<br>\n",
    "I thought about several ideas. However, I explored three main ideas:\n",
    "\n",
    "<pre style=\"font-family:arial;\">\n",
    "1. Predicting currency fx changes to maximise buys and sells.\n",
    "As I have two children in university in Canada the fx rate for USD to CAD is always top of mind.  After exploring this for a bit the challenge to understand the market conditions that I could use for making predictions did not seem to fit well with what I needed for this project and collecting the data was to burdensome for the scope of this project.\n",
    "2. Flight delays, cancellations and the average compensation.  Are the airlines \"gaming\" the system to not pay-out customers given the turmoil in travel I thought it would be interesting to compare recent cancellations, delays and reasons and compensations vs. pre-covid times. Researching for datasets I could not find anything current. Although, there are some pay sites which may have data.  As I would be required pay for the data I decided against this topic.\n",
    "3. COVID data.  This idea would have plenty of source data out there but would it offer the ability to make predictions and not just forecasting trends. I feel this lends it's self to the machine learning section as there are so many stories about the disease so would give me an opportunity to show case what I learned.\n",
    "</pre>\n",
    "\n",
    "<h6>COVID Data</h6>\n",
    "   I picked the COVID idea as there is good data and the types of calculations and techniques required would lend itself to the project with some work. This data is something we are all very familiar with at this time. Governments, countries, organizations and corporations have struggled with rules and regulations trying to balance controlling the epidemic vs. economic stability.\n",
    "\n",
    " I reviewed a couple of sources and in the end selected \"Our World In Data\" (OWID).  OWID has a comprehensive set of publicly available data specifically for COVID.  In working with the FIL business continuity team, I assited with the COVID response. I came across this data source and found it very useful. In the end this is the source we used to provide global situational updates for the senior members in the organization so they could decide on stay at home and return to office responses for each jurisdiction across the organization.  The data is quite clean so I was concerned about showing the data cleaning side of things\n",
    "\n",
    "source of covid data: https://github.com/owid/covid-19-data/tree/master/public/data \n",
    "<p> Originally I downloaded a (.csv) copy of the data to use but the file was large (I was getting an error that the file was to big for my type of GitHub repository account). this occured when I pushed the data to my GitHub repository.  I then researched how I could link to an external csv file, and this solved the problem.  This file creats the opportunity to use current data.  However, I noticed that the most current days data is not 100% populated so I have adjusted to used the most recent data - two days.</p>\n",
    "\n",
    "source of GDP data: https://data.worldbank.org/indicator/NY.GDP.MKTP.CD?year_high_desc=false\n",
    "<p>the file is a zip file which is difficult to connect to so in this instance I downloaded the file and unzipped it.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6f182",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#00008b;\">Implementation Process</h2>\n",
    "(describe your entire process in detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fd799",
   "metadata": {},
   "source": [
    "<h6>Hypothesis</h6>\n",
    "My hypothesis is that countries with higher population density and lower GDP have higher mortality rates than for higher density higher GDP countries. It would also be interesting to see how lower density and higher GDP countries fared and if density and GDP are a predictor of mortaility for a disease like COVID\n",
    "\n",
    "<pre style=\"font-family:arial;\">\n",
    "The implementation process I followed was\n",
    "    Gather Data\n",
    "    Transform & clean\n",
    "    Explore\n",
    "    Analyze and build models\n",
    "</pre\n",
    ">    \n",
    "<h6>Gather Data</h6>\n",
    "There are several measures I need for my analysis. If any of the data sets include 0 values for total I will use the prior days data as total are cumlative.  However, the earlier dates would certainly have 0 as there may not have been cases for a country at the start date of teh data.\n",
    "<pre style=\"font-family:arial;\">\n",
    "Planned measures for each country:\n",
    "    Highest Cases per 100k people: for year end 2020, 2021 and latest 2022\n",
    "    Highest Deaths per 100k people: for year end 2020, 2021 and latest 2022\n",
    "    Lowest Cases per 100k people: for year end 2020, 2021 and latest 2022\n",
    "    Lowest Deaths per 100k people: for year end 2020, 2021 and latest 2022\n",
    "    Look at the 14 day rolling average cases per 100k people over time\n",
    "    Look at the 14 day rolling average deaths per 100k people over time\n",
    "    Population density\n",
    "    GDP per person\n",
    "    Mortality: high/low when cases per 100k deaths reaches a threshold to be set\n",
    "</pre>\n",
    "<h6>Transform & Clean and Explore</h6>\n",
    "Review data for size and complexity, NaNs and missing values. Use techiques like \n",
    "<pre style=\"font-family:arial;\">\n",
    "    .head()\n",
    "    .tail()\n",
    "    .info()\n",
    "    .shape()\n",
    "    .isna().sum()\n",
    "I will need to understand the number of columns, count of records and the type of objects being used: like strings, dates, intergers and floats. I will review the null records and get a count to understand the completeness of the data, and build functions to assit with exploring the data; like creating a rolling n day average and calulation for the total on a per 100,00 of the population for comparative purposes.\n",
    "</pre>\n",
    "\n",
    "<h6>Analyze and build models</h6>\n",
    "Take the top 20: \n",
    "    Categorize as High, Low for mortality and add to the data set.  This will allow some of the linear regression models for correlations\n",
    "\n",
    "Run agaisnt the machine learnng logic for insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4af6aa",
   "metadata": {},
   "source": [
    "<h3>Import and review the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6925ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages needed for project:\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "#from sklearn.module import Model\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e9b1d",
   "metadata": {},
   "source": [
    "<h5>create global variables</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37bb56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many columns are too many to wrangle\n",
    "column_count_limit=30 #number of columns deemed to be managble for exploring data\n",
    "#this will allow a use to run a calculation to high light if a detset has a large number of columns\n",
    "\n",
    "#number of days used in rolling average default = 14 but user could change to 25,30 etc. depending on what timeframe \n",
    "#is relevant\n",
    "days_calc = 14 #n days for calculations. \n",
    "\n",
    "top_n_parameter = 10 #variable to use for select the number of top and bottom records\n",
    "\n",
    "pop_per_100k = 100000 #varibale to set for total cases and deaths per population\n",
    "\n",
    "#for calculations relating to mortality\n",
    "high_deaths_per_100k = 50 # was 50\n",
    "low_deaths_per_100k = 10 # was 10\n",
    "# I decide on this after reviewing the min and maxk values for the topn records for each year end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806c79d",
   "metadata": {},
   "source": [
    "As I create variables used in multiple places I endavour to capture them here so I can easily make updates. This saves time when when making changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e5fa7",
   "metadata": {},
   "source": [
    "<h4>Gather data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e9f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import COVID data\n",
    "\n",
    "# Link and download COVID dataset from OWID\n",
    "url = \"https://covid.ourworldindata.org/data/owid-covid-data.csv\" \n",
    "download = requests.get(url).content\n",
    "\n",
    "# Create the COVID as a pandas dataframe\n",
    "covid_data_raw = pd.read_csv(io.StringIO(download.decode('utf-8')),parse_dates=['date']) #need to parse the date field\n",
    "#source: https://stackoverflow.com/questions/59004960/converting-date-format-in-a-dataframe-from-a-csv-file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd02795",
   "metadata": {},
   "source": [
    "Review of covid header details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c784a",
   "metadata": {},
   "source": [
    "A quick review shows there are a lot of columns of which most will be irrelivant for this work.  There are also records with NaN which will have to be dealt with as they would impact calulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00703336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import World Bank GDP data\n",
    "    # source: https://data.worldbank.org/indicator/NY.GDP.MKTP.CD?year_high_desc=false\n",
    "\n",
    "    # Create the GDP raw file as a pandas dataframe, headers start on row 4\n",
    "gdp_data_raw = pd.read_csv(\"/Users/Phillip/UDCPA_PhillipMarsh/data/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_4489151.csv\", skiprows=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b0108d",
   "metadata": {},
   "source": [
    "Review of global gdp details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e1f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a43ae",
   "metadata": {},
   "source": [
    "A quick review shows there are also alot of columns for yeara most of which would not be relevant.  This data also uses 3 digit ISO codes which means I can use it to join to data if need be to the COVID data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f7ff5",
   "metadata": {},
   "source": [
    "Create global calculations to be used in the analysis\n",
    "\n",
    "There are a few calculations that will be used repeatedly and it makes sense to put them at the start of the project so they are easy to find if changes need to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfeebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global calculation\n",
    "\n",
    "# What are the range of dates in data\n",
    "beg_date = min(covid_data_raw[\"date\"]) #starting point of the available data\n",
    "end_date = max(covid_data_raw[\"date\"]) #most recent data in the file\n",
    "\n",
    "#calculate the lastest observation form the covid data, this data is dynamic and new data comes in each day, however, \n",
    "#it can take time for new data to roll in.  This report is using the last date available less 2 days \n",
    "last_date = end_date - timedelta(2) \n",
    "last_date_n = str(last_date)\n",
    "\n",
    "print(\"The COVID data starts on \"+str(beg_date)+\" and the most recent date is \"+str(end_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfecf06",
   "metadata": {},
   "source": [
    "<h5>Exploring the data</h5>\n",
    "\n",
    "Review the headers, number of headers, type of data to undestand more about the data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723867c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of a dataframe with comment before and after\n",
    "\n",
    "def name_obj(df, comment, comment2=\"\"):\n",
    "    \"\"\"Create statement naming the dataframe around comment and comment2\n",
    "    \n",
    "    Args:\n",
    "        df (dataFrame): the name of the dataframe\n",
    "        comment (string): comment string which goes before the name of the dataframe\n",
    "        comment2 (string): comment string which goes after the name of the dataframe (optional)\n",
    "    \"\"\"\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return (comment+name+comment2)\n",
    "\n",
    "covid_data_raw_name = name_obj(covid_data_raw,\"Dataframe Name is:\")\n",
    "gdp_data_raw_name = name_obj(gdp_data_raw,\"Dataframe Name is:\")\n",
    "\n",
    "#example: test the function\n",
    "print(\"There are two primary sourced datasets used in this project:\")\n",
    "print(covid_data_raw_name)\n",
    "print(gdp_data_raw_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50bffe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create functions for reviewing dataframe headers\n",
    "\n",
    "# create a function to make list from the column header names of a dataframe\n",
    "def column_headers_list(df):\n",
    "    \"\"\"create a list of column headers\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): the name of the dataframe to use\n",
    "    \n",
    "    Returns:\n",
    "        list of column headers\n",
    "    \"\"\"\n",
    "    \n",
    "    columns_lst = df.columns.tolist() # create a list of the column headers from the dataframe\n",
    "        \n",
    "    return columns_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of items in the list from the column header names list of a dataframe\n",
    "\n",
    "#test the function \"column_headers_list\"\n",
    "\n",
    "# Raw Covid data \n",
    "columns_lst_test = column_headers_list(covid_data_raw)\n",
    "columns_len_test = len(columns_lst_test)\n",
    "\n",
    "# Test function\n",
    "#print(columns_lst_test)\n",
    "#print(columns_len_test)\n",
    "\n",
    "\n",
    "# Raw gdp data \n",
    "columns_lst_test = column_headers_list(gdp_data_raw)\n",
    "#print(\"There are :\"+str(columns_len_test)+\" header records\")\n",
    "\n",
    "# Test function\n",
    "#print(columns_lst_test)\n",
    "#print(\"There are :\"+str(columns_len_test)+\" header records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5cfc2f",
   "metadata": {},
   "source": [
    "I # out any testing logic here once the logic was working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function determine if the data set is too wide\n",
    "def columns_comment(xlist,column_count_limit=30):\n",
    "    \"\"\"Use column_len to decide if the dataframe is too large to manage\n",
    "    \n",
    "    Args:\n",
    "        xlist(list): list to review\n",
    "        columns_len(int): from column_headers_list function\n",
    "        column_count_limit(float): limi number of columns to compare\n",
    "    \"\"\"\n",
    "    columns_len = len(xlist)\n",
    "    \n",
    "    if columns_len>column_count_limit:\n",
    "        comment = \"There are many columns (\"+str(columns_len)+\"), Drop a some of them to imporve performance and the size of the file\"\n",
    "    else:\n",
    "        comment = \"Number of columns appears manageable\"\n",
    "    \n",
    "    return comment, columns_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94936489",
   "metadata": {},
   "source": [
    "From the tests I can see both datasets contain quite alot of columns with data, so I will look at each source specifically the COVID data as this source will be critical to the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de42393",
   "metadata": {},
   "source": [
    "<h6>COVID Raw Data</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review Covid raw data\n",
    "\n",
    "# show the column headers and the number of columns\n",
    "\n",
    "data = covid_data_raw\n",
    "\n",
    "columns_len = data.shape[1] # count the number of columns in the list\n",
    "\n",
    "covid_name = name_obj(data,\"The headers from the\", \"Dataframe are:\")\n",
    "print(column_headers_list(data))\n",
    "\n",
    "print()\n",
    "\n",
    "description_covid_raw = name_obj(data,\"The \",\" DataFrame has \"+str(columns_len)+\" columns\")\n",
    "#print(description_covid_raw)\n",
    "\n",
    "print()\n",
    "\n",
    "#print(columns_comment(column_headers_list(df))) #xlist,column_count_limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test function columns_comment()\n",
    "\n",
    "# test for covid data\n",
    "columns_lst_covid = column_headers_list(covid_data_raw) #list of headers\n",
    "\n",
    "comment_covid = columns_comment(columns_lst_covid,)[0] #Comment string\n",
    "header_len_covid = columns_comment(columns_lst_covid,column_count_limit=30)[1] #Number of items in list\n",
    "\n",
    "print(\"for the COVID raw file\")\n",
    "print(comment_covid)\n",
    "\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for gdp data\n",
    "columns_lst_gdp = column_headers_list(gdp_data_raw)\n",
    "\n",
    "comment_gdp = columns_comment(columns_lst_gdp)[0]\n",
    "header_len_gdp = columns_comment(columns_lst_gdp,column_count_limit=30)[1]\n",
    "\n",
    "print(\"for the gdp raw file\")\n",
    "print(comment_gdp)\n",
    "\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6cd17",
   "metadata": {},
   "source": [
    "use the shape function to summarize the total number of rows and columns for each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the data \n",
    "\n",
    "# Information (shape) on are the records + columns\n",
    "\n",
    "# covid raw data\n",
    "print(\"The COVID data shape shows:\")\n",
    "print(covid_data_raw.shape)\n",
    "print()\n",
    "print(\"The GDP data shape shows:\")\n",
    "# gdp raw data\n",
    "print(gdp_data_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6334cc7",
   "metadata": {},
   "source": [
    "There a lot of records for the COVID data, we should limit the number of days to review, but lets remove many of the columns and create a new covid_data DataFrame from the raw file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb260d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# drop columns\n",
    "    #source: https://datatofish.com/drop-columns-pandas-dataframe/#:~:text=Here%20is%20the%20approach%20that%20you%20can%20use,Python%20code%20to%20drop%20the%20%E2%80%98Shape%E2%80%99%20column%20is%3A\n",
    "covid_data = covid_data_raw.drop([\n",
    "    'continent',\n",
    "    'new_cases_smoothed',\n",
    "    'new_deaths_smoothed',\n",
    "    'new_cases_smoothed_per_million',\n",
    "    'new_deaths_smoothed_per_million',\n",
    "    'icu_patients_per_million',\n",
    "    'hosp_patients',\n",
    "    'hosp_patients_per_million',\n",
    "    'weekly_icu_admissions',\n",
    "    'weekly_icu_admissions_per_million',\n",
    "    'weekly_hosp_admissions',\n",
    "    'weekly_hosp_admissions_per_million',\n",
    "    'total_tests_per_thousand',\n",
    "    'new_tests_per_thousand',\n",
    "    'new_tests_smoothed',\n",
    "    'tests_per_case',\n",
    "    'tests_units',\n",
    "    'new_vaccinations_smoothed',\n",
    "    'total_vaccinations_per_hundred',\n",
    "    'people_vaccinated_per_hundred',\n",
    "    'people_fully_vaccinated_per_hundred',\n",
    "    'total_boosters_per_hundred',\n",
    "    'new_vaccinations_smoothed_per_million',\n",
    "    'new_people_vaccinated_smoothed',\n",
    "    'new_people_vaccinated_smoothed_per_hundred',\n",
    "    'stringency_index','median_age',\n",
    "    'aged_65_older',\n",
    "    'aged_70_older',\n",
    "    'cardiovasc_death_rate',\n",
    "    'diabetes_prevalence',\n",
    "    'female_smokers',\n",
    "    'male_smokers',\n",
    "    'handwashing_facilities',\n",
    "    'hospital_beds_per_thousand',\n",
    "    'life_expectancy',\n",
    "    'human_development_index',\n",
    "    'excess_mortality_cumulative_absolute',\n",
    "    'excess_mortality_cumulative',\n",
    "    'excess_mortality',\n",
    "    'excess_mortality_cumulative_per_million',\n",
    "    'total_cases_per_million',\n",
    "    'new_cases_per_million',\n",
    "    'total_deaths_per_million',\n",
    "    'new_deaths_per_million',\n",
    "    'reproduction_rate',\n",
    "    'people_vaccinated',\n",
    "    'total_boosters',\n",
    "    'new_vaccinations',\n",
    "    'new_tests_smoothed_per_thousand',\n",
    "    'new_tests',\n",
    "    'positive_rate'\n",
    "    ], \n",
    "    axis=1)\n",
    "\n",
    "covid_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baadaf86",
   "metadata": {},
   "source": [
    "Using the .info() function we now have 14 columns that look relevant to the analysis, how many specific countries are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f74cc3f",
   "metadata": {},
   "source": [
    "I will create a function that shows unique attributes in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all but the first three columns are float objects; two: \"iso_code\", \"location\" are country specific categories\n",
    "# create a function to get a list of unique values\n",
    "\n",
    "# Function to get unique values\n",
    "  \n",
    "  \n",
    "def unique(list1):\n",
    "  \n",
    "    # Print directly by using * symbol\n",
    "    print(*Counter(list1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eea430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list of covid countrie ISO code\n",
    "\n",
    "# sort by ISO_code and Date\n",
    "covid_data = covid_data.sort_values(['iso_code', 'location', 'date'])\n",
    "\n",
    "#create a list of country codes from the covid data\n",
    "Country_lst_covid_1 = covid_data[\"iso_code\"].tolist()\n",
    "\n",
    "# list the country codes    \n",
    "country_iso_list = unique(Country_lst_covid_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15008856",
   "metadata": {},
   "source": [
    "Review of this object shows there are some ISO_Codes that are more than the standard 3 char length, these should be reviewed. These are related to OWID codes used for regional aggregations of country data, they can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db738756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the \"OWID_\" records\n",
    "\n",
    "# how many records have the OWID ISO_Code?\n",
    "covid_data_owid = covid_data[covid_data[\"iso_code\"].str.contains(\"OWID\")] #OWID data\n",
    "print(\"OWID data shape: \"+str(covid_data_owid.shape))\n",
    "\n",
    "covid_data = covid_data[covid_data[\"iso_code\"].str.contains(\"OWID\")==False] #Non OWID data\n",
    "print(\"Non OWID data shape: \"+str(covid_data.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b653bb",
   "metadata": {},
   "source": [
    "We can see fewer records. I have kept a copy of the OWID aggregate data in case there is time to look at this data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the country ISO Codes again\n",
    "Country_lst_covid_1 = covid_data[\"iso_code\"].tolist()\n",
    "\n",
    "#re-run the unique records; OWID records are no longer displayed\n",
    "country_ISO_list = unique(Country_lst_covid_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d46e56",
   "metadata": {},
   "source": [
    "This look better the 3 digit codes line up nicely and all look correct now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the column headers and the number of columns\n",
    "\n",
    "df_head = covid_data\n",
    "\n",
    "columns_len = df_head.shape[1] # count the number of columns in the list\n",
    "\n",
    "#print(name_obj(df_head,\"The headers from the \", \" DataFrame are:\"))\n",
    "#column_headers_list(df_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a summary of covid_data\n",
    "\n",
    "# Summary of covid_data_raw file\n",
    "rows = covid_data.shape[0]\n",
    "cols = covid_data.shape[1]\n",
    "\n",
    "print(\"The raw data file has {} rows of data\".format(f\"{rows:,d}\"),\"and {} columns\".format(f\"{cols:,d}\"))\n",
    "if cols>column_count_limit:\n",
    "    print(\"There are many columns, drop a some of them to imporve performance and the size of the file\")\n",
    "else:\n",
    "    print(\"Number of columns appears manageable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d646da5",
   "metadata": {},
   "source": [
    "Let mes deal with the NaN (null) data next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe0904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covid_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2279761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covid_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9598a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review Null data\n",
    "# pd.set_option('display.max_rows',None)\n",
    "print(\"Null data:\")\n",
    "print(covid_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce246af5",
   "metadata": {},
   "source": [
    "Review of the columns and null data shows iso_code, loaction (country), date, population are fully populated.\n",
    "\n",
    "population density is not populated for everything, will need to confirm that for the selected date that his is improved\n",
    "\n",
    "total_cases, new_cases, total_deaths, new_deaths etc I would expect null data as data would not be available for all \n",
    "countries from the start of the data period, I will need to convert these to 0's\n",
    "\n",
    "Finally I need to review \"gdp_per_capita\" and \"population_density\" data as that could impact report later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1117a10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Which fields have nan's\n",
    "covid_data[covid_data.isna().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ad780",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# review \"gdp_per_capita\" and \"population_density\"\n",
    "\n",
    "covid_data_gdp_pop = covid_data[[\"date\",\"iso_code\",\"location\",\"population\",\"population_density\",\"gdp_per_capita\"]]\n",
    "\n",
    "covid_data_gdp_pop[\"population\"].isnull()\n",
    "\n",
    "#covid_data_gdp_pop[covid_data_gdp_pop.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587000a2",
   "metadata": {},
   "source": [
    "As I only plan on using certain dates, specifically year end and the most recent for the current year, dropping these NaN records should not impact the report much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847ec94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the NaN for the population and gdp columns\n",
    "covid_data_before_na = covid_data # allows me to have access to pre change data\n",
    "covid_data = covid_data.dropna(subset=[\"population\",\"population_density\",\"gdp_per_capita\"])\n",
    "#https://www.datasciencelearner.com/pandas-dropna-remove-nan-rows-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review NaN data after removing records\n",
    "covid_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86afaa0",
   "metadata": {},
   "source": [
    "Next I look at the value \"total\" columns that have NaNs that should really be 0s.  Many of these NaNs are form the earlier dates when there weren't many cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the NaN's to 0 for the remainder valuations\n",
    "covid_data = covid_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1941dea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "covid_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a0004",
   "metadata": {},
   "source": [
    "Final data set looks good and complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968756a",
   "metadata": {},
   "source": [
    "<h6>Final review of the COVID dataset</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d54eadf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many records am I dealing with\n",
    "\n",
    "#total_records = covid_data.count(axis=1)\n",
    "#print(total_records)\n",
    "#print(\"\")\n",
    "\n",
    "# show the countries/ locations in the data\n",
    "print(\"ISO codes and Country\")\n",
    "print(covid_data.pivot_table(index = [\"iso_code\", \"location\"], aggfunc =\"size\"))\n",
    "\n",
    "print(\"\")\n",
    "# df.size\n",
    "print(\"Size:\")\n",
    "print(covid_data.size)\n",
    "\n",
    "print(\"\")\n",
    "# df.isnull()\n",
    "column_picker =\"total_deaths\"\n",
    "covid_ttl_deaths = covid_data.filter([\"iso_code\", \"location\",column_picker])\n",
    "bool_series_null =pd.isnull(covid_ttl_deaths[column_picker])\n",
    "\n",
    "print(\"Null\",column_picker,\": \")\n",
    "print(covid_ttl_deaths[bool_series_null])\n",
    "#print(covid_data.isnull())\n",
    "\n",
    "print(\"\")\n",
    "# df.notnull()\n",
    "bool_series = pd.notnull(covid_ttl_deaths[column_picker])\n",
    "print(\"Not null:\")\n",
    "print(covid_ttl_deaths[bool_series])\n",
    "\n",
    "print(\"\")\n",
    "# df.describe()\n",
    "print(\"Describe:\")\n",
    "print(covid_data.describe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69826753",
   "metadata": {},
   "source": [
    "Looks much better there are now no nulls for the Total_deaths which will allow caculations to be performed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47caa9",
   "metadata": {},
   "source": [
    "<h6>GDP Raw Data</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f0387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# review GDP data\n",
    "\n",
    "#look at info for gdp data\n",
    "print(\"shape of the GDP raw data:\")\n",
    "print(gdp_data_raw.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c653bca",
   "metadata": {},
   "source": [
    "First thing we can drop many of the year columns as the covid data does not go back that far. Clean up the column headers so they are consistent with the COVID data i.e no spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c69e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not need most of the columns so will remove cols 4:63\n",
    "gdp_data = gdp_data_raw.drop(gdp_data_raw.iloc[:,4:63],axis = 1)\n",
    "\n",
    "#gdp_data = gdp_data_1.drop(gdp_data_raw.iloc[:,7],axis = 1)\n",
    "\n",
    "#convert the spaces \" \" to underscore \"_\" consitent with the COVID data\n",
    "gdp_data.columns = [c.replace(' ', '_') for c in gdp_data.columns]\n",
    "\n",
    "gdp_data.drop('Unnamed:_66', axis=1, inplace=True)\n",
    "\n",
    "print(gdp_data.info())\n",
    "print()\n",
    "print(gdp_data.head())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the column headers and the number of columns\n",
    "\n",
    "columns_len = gdp_data.shape[1] # count the number of columns in the list\n",
    "gdp_data_name = name_obj(gdp_data,\"The headers from the \", \" Dataframe are:\")\n",
    "\n",
    "print(gdp_data_name)\n",
    "columns_lst_gdp = column_headers_list(gdp_data)\n",
    "print(columns_lst_gdp)\n",
    "\n",
    "print()\n",
    "\n",
    "print(name_obj(gdp_data,\"The \",\" DataFrame has \"+str(columns_len)+\" columns\"))\n",
    "\n",
    "print()\n",
    "\n",
    "#print(columns_comment())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adeceee",
   "metadata": {},
   "source": [
    "GDP data looks much better and is ready if I need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations of the gdp_data\n",
    "gdp_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817b874",
   "metadata": {},
   "source": [
    "<h4>Summary of data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137280a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Summary of Covid data\n",
    "print(\"summary of Covid data\")\n",
    "print()\n",
    "\n",
    "# Number of unique countries\n",
    "n = covid_data.iso_code.nunique()\n",
    "print(\"No of unique countries (covid_data):\",n)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Number of unique dates\n",
    "n = covid_data.date.nunique()\n",
    "\n",
    "\n",
    "\n",
    "print(\"No of unique dates: \",n)\n",
    "print(\"From: \",beg_date.strftime(\"%b %d %Y\"),\" to: \",end_date.strftime(\"%b %d %Y\"))\n",
    "print(\"\")\n",
    "\n",
    "# Number of records\n",
    "rec = covid_data.shape[0]\n",
    "col = covid_data.shape[1]\n",
    "\n",
    "print(\"No of rows: \",f\"{rec:,d}\")\n",
    "print(\"No of columns: \",f\"{col:,d}\") \n",
    "#source: https://stackoverflow.com/questions/60934535/format-integer-with-comma-using-python-printf\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of GDP data\n",
    "\n",
    "print(\"summary of GDP Data\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Number of unique countries\n",
    "n = gdp_data.Country_Code.nunique()\n",
    "print(\"No of unique countries: \",n)\n",
    "print(\"\")\n",
    "\n",
    "# Number of records\n",
    "rec = gdp_data.shape[0]\n",
    "col = gdp_data.shape[1]\n",
    "\n",
    "print(\"No of rows: \",f\"{rec:,d}\")\n",
    "print(\"No of columns: \",f\"{col:,d}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e730c77",
   "metadata": {},
   "source": [
    "<h6>calculations for reporting</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492a77b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculations for report:\n",
    "\n",
    "# date calculations\n",
    "# There needs to be a n_day (number of days) total for certain total columns to in turn calculate \n",
    "# comparative data against 100k of a countries population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966defc4",
   "metadata": {},
   "source": [
    "First create a rolling n_days average per 100k of teh population. I need use the record \"date\" and go back 14days. Except if the date is with in 14days of the start of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n day calculations can't begin until the nth day after the first date in the dataset (beg_date)\n",
    "first_calc_date = beg_date +  timedelta(days=days_calc)\n",
    "print(\"Begining Date: \"+str(beg_date)+\"; Earliest starting date for calculations: \"+str((first_calc_date)))\n",
    "\n",
    "# calculate the start date for the n days data for each record\n",
    "n_day_start = covid_data[\"date\"] - timedelta(days=days_calc)\n",
    "print()\n",
    "print(\"show that the dates are populating with different results\")\n",
    "print(n_day_start)\n",
    "print(\"its working\")\n",
    "print()\n",
    "\n",
    "# Insert a column with the n day start date, this shows when the n days rolling date can start calculating\n",
    "covid_data.insert(loc=3, column=\"n_day_start_date\", value=n_day_start, allow_duplicates=False) \n",
    "#false will not allow the column to be entered more than once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e408f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(covid_data.info())\n",
    "print()\n",
    "print(\"the new column is now appearing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8fb5c",
   "metadata": {},
   "source": [
    "Create the n_rolling days functions and insert into the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58a990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n days totals \n",
    "# (https://stackoverflow.com/questions/28236305/how-do-i-sum-values-in-a-column-that-match-a-given-condition-using-pandas)\n",
    "# https://python.tutorialink.com/calculate-14-day-rolling-average-on-data-with-two-hierarchies/\n",
    "\n",
    "covid_data.sort_values(['iso_code','date'], ascending=(True,True), inplace=True)\n",
    "\n",
    "# Rolling new cases \n",
    "rolling_new_cases = covid_data.groupby(['iso_code'])['new_cases'].transform(lambda x: x.rolling(days_calc, 1).sum())\n",
    "\n",
    "# Insert a column with the \"n\" rolling new cases\n",
    "\n",
    "#new_column string name\n",
    "new_column = str(days_calc)+\"_days_rolling_new_cases\"\n",
    "print(new_column)\n",
    "\n",
    "print(new_column in covid_data.columns) # Test for existing column# True\n",
    "\n",
    "#  delete new column, use if re-runing with out resetting the data, \n",
    "#if time allows will create if statement to check if column is available then deleting else insert the column\n",
    "#del covid_data[str(days_calc)+\"_days_rolling_new_cases\"]\n",
    "\n",
    "# insert new_column\n",
    "covid_data.insert(loc=6, column=str(days_calc)+\"_days_rolling_new_cases\", value=rolling_new_cases, allow_duplicates=False)\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Rolling new deaths \n",
    "rolling_new_deaths = covid_data.groupby(['iso_code'])['new_deaths'].transform(lambda x: x.rolling(days_calc, 1).sum())\n",
    "\n",
    "\n",
    "# Insert a column with the \"n\" rolling new deaths\n",
    "\n",
    "#new_column string name\n",
    "new_column_2 = str(days_calc)+\"_days_rolling_new_deaths\"\n",
    "print(new_column_2)\n",
    "\n",
    "print(new_column_2 in covid_data.columns) # Test for existing column# True\n",
    "\n",
    "# delete new column\n",
    "#del covid_data[str(days_calc)+\"_days_rolling_new_deaths\"]\n",
    "\n",
    "# insert new_column\n",
    "covid_data.insert(loc=9, column=str(days_calc)+\"_days_rolling_new_deaths\", value=rolling_new_cases, allow_duplicates=False)\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "\n",
    "#repeat for new deaths\n",
    "#still need to create calculations for: \n",
    "    #total_cases_per_100k per 100k of the population (total_cases/population * 100,000)\n",
    "    #total_deaths_per_100k of the population (total_deaths/population * 100,000)\n",
    "    #total_cases_per_100sqkm of the country (total_cases/total country sqkm *100,000)\n",
    "    #total_deaths_per_100sqkm of the country (total_deaths/total country sqkm *100,000)\n",
    "#these will be used to use machine learning to establish if the GDP or pop density had an impact on the mortality\n",
    "#merge in the gdp data if required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f1afc",
   "metadata": {},
   "source": [
    "Create the total_100k of the population so we can compare countries if need be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef22a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cases per 100K of population \n",
    "total_cases_per_100k = covid_data.total_cases/covid_data.population * pop_per_100k\n",
    "\n",
    "# Insert a column total cases per 100k\n",
    "\n",
    "#new_column string name\n",
    "new_column_3 = \"total_cases_per_100k\"\n",
    "print(new_column_3)\n",
    "\n",
    "print(new_column_3 in covid_data.columns) # Test for existing column# True\n",
    "\n",
    "#  delete new column\n",
    "#del covid_data[\"total_cases_per_100k\"]\n",
    "\n",
    "# insert new_column\n",
    "covid_data.insert(loc=6, column=\"total_cases_per_100k\", value=total_cases_per_100k, allow_duplicates=False)\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a634ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deaths per 100K of population \n",
    "total_deaths_per_100k = covid_data.total_deaths/covid_data.population * pop_per_100k\n",
    "\n",
    "# Insert a column total deaths per 100k\n",
    "\n",
    "#new_column string name\n",
    "new_column_4 = \"total_deaths_per_100k\"\n",
    "print(new_column_4)\n",
    "\n",
    "print(new_column_4 in covid_data.columns) # Test for existing column# True\n",
    "\n",
    "#  delete new column\n",
    "#del covid_data[\"total_deaths_per_100k\"]\n",
    "\n",
    "# insert new_column\n",
    "covid_data.insert(loc=6, column=\"total_deaths_per_100k\", value=total_deaths_per_100k, allow_duplicates=False)\n",
    "\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f099dc9",
   "metadata": {},
   "source": [
    "For the regressions I need to create a subset of the data to look at\n",
    "\n",
    "Take the data for year end for 2020, 2021 and the most recent data from 2022\n",
    "review this data and create the top deaths and the bottom deaths sets of data.\n",
    "In the top_n death set look at the minimum value to set the lower threshold for the \"HigH\" mortalility classificaion\n",
    "In the bottom_n deaths look at the max value to set the upper threshold for the \"Low\" mortalility classificaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba94b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a classification for mortality if the total_deaths per 100k is high or low\n",
    "# obesrvations of the deaths for 2020 and 2021, get the min value of the top records:\n",
    "#covid_data[covid_data[\"date\"].isin([\"2020-12-31\",\"2021-12-31\",\"2022-09-15\"])]#,last_date])]\n",
    "\n",
    "#last_date_n = str(last_date_n = str(last_date))\n",
    "\n",
    "# filter data for the dates:\n",
    "covid_data_observe = covid_data[covid_data[\"date\"].isin([\"2020-12-31\",\"2021-12-31\",last_date_n])]\n",
    "covid_data_observe_20_21 =covid_data[covid_data[\"date\"].isin([\"2020-12-31\",\"2021-12-31\"])]\n",
    "covid_data_observe_22 =covid_data[covid_data[\"date\"].isin([last_date_n])]\n",
    "\n",
    "#top and bottom observations\n",
    "top_20_21 = covid_data_observe_20_21.nlargest(n=top_n_parameter, columns=[\"total_deaths_per_100k\"])\n",
    "bot_22 = covid_data_observe_22.nsmallest(n=top_n_parameter, columns=[\"total_deaths_per_100k\"])\n",
    "\n",
    "# min value in the Top mortality (top deaths) data\n",
    "print(\"min of 2020/21 top deaths/ 100k: \"+str(top_20_21[\"total_deaths_per_100k\"].min()))\n",
    "print(\"max of 2020/21 top deaths/ 100k: \"+str(top_20_21[\"total_deaths_per_100k\"].max()))\n",
    "print(\"these records look very high, it could be due to an outlier, I have recalculted below again once more of the data is cleaned\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# max value in the bottom mortality (bottom deaths) data\n",
    "print(\"max of 2022 lowest deaths/ 100k: \"+str(bot_22[\"total_deaths_per_100k\"].max()))\n",
    "print(\"min of 2022 lowest deaths/ 100k: \"+str(bot_22[\"total_deaths_per_100k\"].min()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba707c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers for deaths beig high should be above 50 per 100k and below 10 per 100k of the population\n",
    "print(\"high_deaths = \"+str(high_deaths_per_100k))\n",
    "print(\"low_deaths = \"+str(low_deaths_per_100k))\n",
    "# set these variables at the top of the project to cahnge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89af1f",
   "metadata": {},
   "source": [
    "Add the classification to the covid_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194f332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a calculation to insert the classification group of \"Low\" (10,25,50) to \"high\"\n",
    "#if the total_deaths_per_100k >= 50 then \"High Deaths\" elseif total_deaths_per_100k <= 10 then \"low Deaths\"\n",
    "\n",
    "covid_data.loc[covid_data[\"total_deaths_per_100k\"] <= low_deaths_per_100k, \"mortality\"] = \"low\" \n",
    "covid_data.loc[covid_data[\"total_deaths_per_100k\"] >= high_deaths_per_100k, \"mortality\"] = \"high\" \n",
    "\n",
    "covid_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d8885",
   "metadata": {},
   "source": [
    "It can be observerd from the .tail function that observations between 10 to 50 will be blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783454b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review covid data using the iso code for a country\n",
    "covid_data_country = covid_data[covid_data[\"iso_code\"]==\"ABW\"]\n",
    "\n",
    "covid_data_country.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da6abc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covid_data_country.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b941e68",
   "metadata": {},
   "source": [
    "I observe the earlier data in the .head() function shows empty mortality data but the later .tail() \"high\"'s can be seen and the matches the logic expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df35ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there any null values introduced by this process\n",
    "print(covid_data.shape)\n",
    "print()\n",
    "print(covid_data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4df55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to balnk out the NaN's created in the mortality description field\n",
    "covid_data = covid_data.fillna(\"\")\n",
    "\n",
    "print(covid_data.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d879e74",
   "metadata": {},
   "source": [
    "All the new columns and records are the same in the datset now. Let's review the .corr() function for some quick insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations of the covid_data\n",
    "covid_data.corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635576b1",
   "metadata": {},
   "source": [
    "This does not look very promising but it's hard to read. Let's review as a heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbc32e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Correlation of population density\n",
    "corr = covid_data.corr()\n",
    "plt.figure(figsize=(20, 9))\n",
    "k = 12 #number of variables for heatmap\n",
    "cols = corr.nlargest(k, 'population_density')['population_density'].index\n",
    "cm = np.corrcoef(covid_data[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,cmap=\"Blues\")\n",
    "plt.title(\"Correlation of population density\", size = 15)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c954489",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Correlation of gdp\n",
    "corr = covid_data.corr()\n",
    "plt.figure(figsize=(20, 9))\n",
    "k = 12 #number of variables for heatmap\n",
    "cols = corr.nlargest(k, 'gdp_per_capita')['gdp_per_capita'].index\n",
    "cm = np.corrcoef(covid_data[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,cmap=\"Blues\")\n",
    "plt.title(\"Correlation of GDP per Capita\", size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72284a",
   "metadata": {},
   "source": [
    "Darker shadeing represent positive correlation.  From this we can infer that population density and gdp are not correlated to the mortality rate of a country.  GDP appears to have slightly better correllation than the population density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51263092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if we need some sort of index key to view the data, I created one I can insert if I need it later\n",
    "pk = covid_data[\"iso_code\"]+str(covid_data['date'])\n",
    "\n",
    "#print(pk.head())\n",
    "\n",
    "#insert pk into covid_data\n",
    "#del covid_data[\"pk\"] #delete pk column\n",
    "#covid_data.insert(0, 'pk', pk)\n",
    "\n",
    "\n",
    "#covid_data[\"pk\"]\n",
    "#covid_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3edf83",
   "metadata": {},
   "source": [
    "Summary of data exploration and preparation:  the data is ready for anlyis but my confidence level is not high after reviewing the .corr() results.  I decided to leave the GDP data source at this point as the COVID GDP per capita data looks to be a good representation of the data.  Next step is to pefrom regession and machine learning, although given the low correlation I am seeing not sure how fruitful it will be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045aefa",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#00008b;\">Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e3d4f",
   "metadata": {},
   "source": [
    "<h3>Basic Charts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set chart sizes to wide\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "#list of iso codes\n",
    "Country_review = covid_data[\"iso_code\"].tolist()\n",
    "\n",
    "# list the country codes    \n",
    "country_iso_list = unique(Country_review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b502aa4",
   "metadata": {},
   "source": [
    "Select a country code above to test the chart below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273af15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "country_use_code = \"GBR\"\n",
    "\n",
    "#covid_data = covid_data.set_index(\"pk\")\n",
    "\n",
    "iso_use = country_use_code\n",
    "\n",
    "print(\"Total Cases vs \"+new_column+\" for: \"+iso_use)\n",
    "\n",
    "\n",
    "# Basic plot of total covid cases over time\n",
    "fig, ax=plt.subplots(2,1, sharey=True)\n",
    "\n",
    "\n",
    "data=covid_data[covid_data[\"iso_code\"]==iso_use]\n",
    "data1=covid_data[covid_data[\"iso_code\"]==iso_use]\n",
    "\n",
    "ax[0].plot(data[\"date\"], data[\"total_cases\"], color='b')\n",
    "print()\n",
    "ax[1].plot(data[\"date\"], data[new_column], color='g') #using the new_column function above that identifies the name of the new column\n",
    "\n",
    "ax[1].set_xlabel(\"Date\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cc6da7",
   "metadata": {},
   "source": [
    "The charts above give an indication of the total mortality per 100k people and rolling 14 day spikes representing the waves over time.  we can see that the total increases sharply between the end of 2020, 2021 and is now leveling off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2ba97",
   "metadata": {},
   "source": [
    "<h3>analysis with seaborn</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39780d0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "I'm going to create a subset of the data as mentioned between year end totals/100k population to compare and see if the gdp, martality calssification impacts the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0265c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a subset of the COVID data for use with seaborn analysis\n",
    "\n",
    "covid_data_small = covid_data[['date',\n",
    "                               'iso_code',\n",
    "                               'location',\n",
    "                               'total_cases',\n",
    "                               'total_cases_per_100k',\n",
    "                               'total_deaths',\n",
    "                               'total_deaths_per_100k',\n",
    "                               'population',\n",
    "                               'population_density',\n",
    "                               'gdp_per_capita',\n",
    "                               'extreme_poverty',\n",
    "                               'people_fully_vaccinated',\n",
    "                               'mortality'\n",
    "                              ]]\n",
    "\n",
    "\n",
    "#covid_data_small.fillna(0)\n",
    "covid_data_small\n",
    "\n",
    "last_date_n = str(last_date)\n",
    "\n",
    "print(\"last date to use: \"+last_date_n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf558b01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter on dates for analysis\n",
    "covid_data_sns = covid_data_small[covid_data_small[\"date\"].isin([\"2020-12-31\",\"2021-12-31\",last_date_n])]\n",
    "\n",
    "print(\"covid_data_sns.shape\") \n",
    "print(covid_data_sns.shape) \n",
    "print(\"-\"*100)\n",
    "print()\n",
    "print(covid_data_sns.head())\n",
    "print(\"-\"*100)\n",
    "print(covid_data_sns.tail())\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "print(\"Null data\")\n",
    "print(covid_data_sns.isna().sum())\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "print(covid_data_sns.corr())\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ddffa",
   "metadata": {},
   "source": [
    "I can see that we are getting multiple dates and requested columns of data back so good to move forward. Mortality is showing high, low and blank values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488455d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"gdp_per_capita\",y=\"population\",data=covid_data_sns)\n",
    "plt.title(\"Scatter plot of GPD per Capita vs Population\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e89e27",
   "metadata": {},
   "source": [
    "Looks like a few outliers with a few high gdp nodes with relatively low populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd117b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"gdp_per_capita\",y=\"total_deaths_per_100k\",data=covid_data_sns)\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.title(\"Scatter plot of GDP per Capita vs. Mortality per 100k\", size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e125fd90",
   "metadata": {},
   "source": [
    "looks like an intersting visual and that we can what appears to be a pattern between countries, GDP and mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data sets for each year\n",
    "df_2020 = covid_data_sns[covid_data_sns[\"date\"].isin([\"2020-12-31\"])]\n",
    "df_2021 = covid_data_sns[covid_data_sns[\"date\"].isin([\"2021-12-31\"])]\n",
    "df_2022 = covid_data_sns[covid_data_sns[\"date\"].isin([\"2022-09-15\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73600e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"gdp_per_capita\",y=\"total_deaths_per_100k\",data=df_2022, hue=\"location\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.title(\"Scatter plot of GDP per Capita vs. Mortality per 100k for 2022\", size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72881c12",
   "metadata": {},
   "source": [
    "only displaying a single point in time gets rid of some noise and it would appear looking at the latest data here that there appears to be a relationship between gdp and mortality rates.  it might be easier to review by the top and bottom countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcd098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top n data; use: top_n_parameter \n",
    "#https://datascientyst.com/get-top-10-highest-lowest-values-pandas/\n",
    "#df.nlargest; df.nsmallest\n",
    "\n",
    "\n",
    "print(\"Top countries by cases and deaths:\")\n",
    "print()\n",
    "\n",
    "df_2020 = covid_data_sns[covid_data_sns[\"date\"].isin([\"2020-12-31\"])]\n",
    "df_2021 = covid_data_sns[covid_data_sns[\"date\"].isin([\"2021-12-31\"])]\n",
    "df_2022 = covid_data_sns[covid_data_sns[\"date\"].isin([last_date_n])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe669d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"creating a sets of top n cases and deaths per 100k of the population\")\n",
    "print()\n",
    "print(\"Bottom countries by cases and deaths:\")\n",
    "print()\n",
    "\n",
    "top_df_2020_cases_per_100k = df_2020.nlargest(n=top_n_parameter, columns=[\"total_cases_per_100k\"])\n",
    "print(\"top_df_2020_cases_per_100k\")\n",
    "print(top_df_2020_cases_per_100k)\n",
    "print(\"-\"*100)\n",
    "\n",
    "top_df_2020_deaths_per_100k = df_2020.nlargest(n=top_n_parameter, columns=[\"total_deaths_per_100k\"])\n",
    "print(\"top_df_2020_deaths_per_100k\")\n",
    "print(top_df_2020_deaths_per_100k)\n",
    "print(\"-\"*100)\n",
    "\n",
    "top_df_2021_cases_per_100k = df_2021.nlargest(n=top_n_parameter, columns=[\"total_cases_per_100k\"])\n",
    "print(\"top_df_2021_cases_per_100k\")\n",
    "print(top_df_2021_cases_per_100k)\n",
    "print(\"-\"*100)\n",
    "\n",
    "top_df_2021_deaths_per_100k = df_2021.nlargest(n=top_n_parameter, columns=[\"total_deaths_per_100k\"])\n",
    "print(\"top_df_2021_deaths_per_100k\")\n",
    "print(top_df_2021_deaths_per_100k)\n",
    "print(\"-\"*100)\n",
    "\n",
    "top_df_2022_cases_per_100k = df_2022.nlargest(n=top_n_parameter, columns=[\"total_cases_per_100k\"])\n",
    "print(\"top_df_2022_cases_per_100k\")\n",
    "print(top_df_2022_cases_per_100k)\n",
    "print(\"-\"*100)\n",
    "\n",
    "top_df_2022_deaths_per_100k = df_2022.nlargest(n=top_n_parameter, columns=[\"total_deaths_per_100k\"])\n",
    "print(\"top_df_2022_deaths_per_100k\")\n",
    "print(top_df_2022_deaths_per_100k)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0d219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"creating a sets of bottom n cases and deaths per 100k of the population\")\n",
    "print()\n",
    "\n",
    "# Bottom n data; use: top_n_parameter \n",
    "#https://datascientyst.com/get-top-10-highest-lowest-values-pandas/\n",
    "#df.nlargest; df.nsmallest\n",
    "print(\"Bottom countries by cases and deaths:\")\n",
    "print()\n",
    "\n",
    "bot_df_2020_cases_per_100k = df_2020.nsmallest(n=top_n_parameter, columns=[\"total_cases_per_100k\"])\n",
    "print(\"bot_df_2020_cases_per_100k\")\n",
    "print(top_df_2020_cases_per_100k)\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "\n",
    "bot_df_2020_deaths_per_100k = df_2020.nsmallest(n=top_n_parameter, columns=[\"total_deaths_per_100k\"])\n",
    "print(\"bot_df_2020_deaths_per_100k\")\n",
    "print(top_df_2020_deaths_per_100k)\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "\n",
    "bot_df_2021_cases_per_100k = df_2021.nsmallest(n=top_n_parameter, columns=[\"total_cases_per_100k\"])\n",
    "print(\"bot_df_2021_cases_per_100k\")\n",
    "print(top_df_2021_cases_per_100k)\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "\n",
    "bot_df_2021_deaths_per_100k = df_2021.nsmallest(n=top_n_parameter, columns=[\"total_deaths_per_100k\"])\n",
    "print(\"bot_df_2021_deaths_per_100k\")\n",
    "print(top_df_2021_deaths_per_100k)\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "\n",
    "bot_df_2022_cases_per_100k = df_2022.nsmallest(n=top_n_parameter, columns=[\"total_cases_per_100k\"])\n",
    "print(\"bot_df_2022_cases_per_100k\")\n",
    "print(top_df_2022_cases_per_100k)\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "\n",
    "bot_df_2022_deaths_per_100k = df_2022.nsmallest(n=top_n_parameter, columns=[\"total_deaths_per_100k\"])\n",
    "print(\"bot_df_2022_deaths_per_100k\")\n",
    "print(top_df_2022_deaths_per_100k)\n",
    "print(\"-\"*100)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47078afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# min value in the Top mortality (top deaths) data\n",
    "print(\"min of 2020 top deaths/ 100k: \"+str(top_df_2020_deaths_per_100k[\"total_deaths_per_100k\"].min()))\n",
    "print(\"max of 2020 top deaths/ 100k: \"+str(top_df_2020_deaths_per_100k[\"total_deaths_per_100k\"].max()))\n",
    "print(\"-\"*100)\n",
    "print(\"min of 2021 top deaths/ 100k: \"+str(top_df_2021_deaths_per_100k[\"total_deaths_per_100k\"].min()))\n",
    "print(\"max of 2021 top deaths/ 100k: \"+str(top_df_2021_deaths_per_100k[\"total_deaths_per_100k\"].max()))\n",
    "print(\"-\"*100)\n",
    "\n",
    "# max value in the bottom mortality (bottom deaths) data\n",
    "print(\"max of 2022 lowest deaths/ 100k: \"+str(bot_df_2022_deaths_per_100k[\"total_deaths_per_100k\"].max()))\n",
    "print(\"min of 2022 lowest deaths/ 100k: \"+str(bot_df_2020_deaths_per_100k[\"total_deaths_per_100k\"].min()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f676dbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Top and Bottom Mortality vs GDP per capita\")\n",
    "print()\n",
    "\n",
    "#Top deaths vs gdp\n",
    "sns.scatterplot(x=\"gdp_per_capita\",y=\"total_deaths_per_100k\",data=top_df_2022_deaths_per_100k, hue=\"location\")\n",
    "plt.title(\"High Deaths per 100k vs GDP, 2022\", size = 15)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.show()\n",
    "\n",
    "#bottom deaths vs gdp\n",
    "sns.scatterplot(x=\"gdp_per_capita\",y=\"total_deaths_per_100k\",data=bot_df_2022_deaths_per_100k, hue=\"location\")\n",
    "plt.title(\"Low Deaths per 100k vs GDP, 2022\", size = 15)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a00d1",
   "metadata": {},
   "source": [
    "this looks strange, high mortality and gdp does not look related.  it appears from the top chart that the higher mortality countries also have higher gdp, for the most part these countries look like smaller nations.  lets look by population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b75a6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Top and Bottom Mortality vs Population Density\")\n",
    "\n",
    "#Top deaths vs population_density\n",
    "sns.scatterplot(x=\"population_density\",y=\"total_deaths_per_100k\",data=top_df_2022_deaths_per_100k, hue=\"location\")\n",
    "plt.title(\"High Deaths per 100k vs Pop. density, 2022\", size = 15)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.show()\n",
    "\n",
    "#bottom deaths vs population_density\n",
    "sns.scatterplot(x=\"population_density\",y=\"total_deaths_per_100k\",data=bot_df_2022_deaths_per_100k, hue=\"location\")\n",
    "plt.title(\"Low Deaths per 100k vs Pop., 2022\", size = 15)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dcfeb0",
   "metadata": {},
   "source": [
    "this looks strange as well, higher mortality and lower density looks negatively related, it's difficult to tell because of the outlier, Macao, in the lower chart.  it appears from the top chart that the lesser dense countries also have higer mortality, for the most part these countries look like smaller nations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f24ba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the threshold to use for the \"high\" and \"low\" mortality column\n",
    "\n",
    "# min value in the Top mortality (top deaths) data\n",
    "print(\"min 2020 top deaths/ 100k: \"+str(top_df_2020_cases_per_100k[\"total_deaths_per_100k\"].min()))\n",
    "print(\"max 2020 top deaths/ 100k: \"+str(top_df_2020_cases_per_100k[\"total_deaths_per_100k\"].max()))\n",
    "print(\"-\"*100)\n",
    "\n",
    "# max value in the bottom mortality (bottom deaths) data\n",
    "print(\"max 2022 lowest deaths/ 100k: \"+str(bot_df_2022_cases_per_100k[\"total_deaths_per_100k\"].max()))\n",
    "print(\"min 2022 lowest deaths/ 100k: \"+str(bot_df_2022_cases_per_100k[\"total_deaths_per_100k\"].min()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54edab7",
   "metadata": {},
   "source": [
    "I used this to set the group bads for the \"Mortality\" column.  Using the 2020 top 10 records to set the lower limit of \"high\" mortality and current 2022 bottom 10 records to set the higher limit for the \"low\" mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b9b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Mortality per 100k and GDP per capita\n",
    "\n",
    "#High mortality vs gdp\n",
    "sns.relplot(x=\"gdp_per_capita\",\n",
    "            y=\"total_deaths_per_100k\",\n",
    "            data=covid_data_sns, \n",
    "            kind=\"scatter\",\n",
    "            col = \"date\")\n",
    "plt.show()\n",
    "\n",
    "#bottom deaths vs gdp\n",
    "#sns.scatterplot(x=\"gdp_per_capita\",y=\"total_deaths_per_100k\",data=bot_df_2020_cases_per_100k, hue=\"location\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c659d",
   "metadata": {},
   "source": [
    "we can observe from this that these are expected results; over time lower gdp per capita records had higher mortality per 100k of the population. However, there are some interesting results where lower gdp did not have a high mortality.  notice the skew to the upper left over time which suggests lower gdp does impact higher mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd728e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Mortality per 100k and population density\n",
    "\n",
    "#High mortality vs pop density\n",
    "sns.relplot(x=\"population_density\",\n",
    "            y=\"total_deaths_per_100k\",\n",
    "            data=covid_data_sns, \n",
    "            kind=\"scatter\",\n",
    "            col = \"date\")\n",
    "plt.show()\n",
    "\n",
    "#bottom deaths vs gdp\n",
    "#sns.scatterplot(x=\"gdp_per_capita\",y=\"total_deaths_per_100k\",data=bot_df_2020_cases_per_100k, hue=\"location\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd82447",
   "metadata": {},
   "source": [
    "we can observe here that a higher density in the population does not have a more significant impact on mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c326f5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "covid_temp = covid_data_sns\n",
    "covid_temp.sort_values(\"population_density\", axis = 0, ascending = False, inplace = True, na_position ='last')\n",
    "covid_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced8fc5",
   "metadata": {},
   "source": [
    "<h3>Regression analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fc7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning KNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data_sns.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = unique(covid_data_sns[\"date\"])\n",
    "print(\"we have the expected 3 dates selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f83fe4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create 2 sets of data for the review removing date, iso code, location, as this was causing an issue with the regression, \n",
    "#and even fewer value columns in the second set \n",
    "\n",
    "covid_data_ml = covid_data_sns\n",
    "\n",
    "covid_data_ml1 = covid_data_ml.drop([\"date\",\"iso_code\",\"location\"],axis=1)\n",
    "\n",
    "covid_data_ml2 = covid_data_ml.drop([\"date\",\"iso_code\",\"location\",\"total_cases\", \"total_deaths\"],axis=1)\n",
    "\n",
    "covid_data_ml2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb505be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covid_data_ml2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3150fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#heat map of the correlations for covid_data_ml2\n",
    "\n",
    "Corr_data = covid_data_ml2\n",
    "\n",
    "#Correlation of gdp\n",
    "corr = Corr_data.corr()\n",
    "plt.figure(figsize=(20, 9))\n",
    "\n",
    "k = 12 #number of variables for heatmap\n",
    "\n",
    "cols = corr.nlargest(k, 'gdp_per_capita')['gdp_per_capita'].index\n",
    "cm = np.corrcoef(Corr_data[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,cmap=\"Blues\")\n",
    "plt.title(\"heat map of the correlations for covid_data_ml\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ad932",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#00008b;\">Results</h2>\n",
    "(Include the charts and describe them)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6183a",
   "metadata": {},
   "source": [
    "<h4>Supervised learning with classification</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training and testing the data\")\n",
    "#from datacamp\n",
    "print()\n",
    "\n",
    "#covid_data_ml\n",
    "#covid_data_ml1\n",
    "#covid_data_ml2 \n",
    "\n",
    "ml_data = covid_data_ml1\n",
    "\n",
    "X = ml_data.drop(\"mortality\",axis=1).values #drop target value\n",
    "y = ml_data[\"mortality\"].values #target observations\n",
    "\n",
    "#split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=21,stratify=y)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "#fit the classiier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#print the accuracy\n",
    "print(\"The knn score:\")\n",
    "print(knn.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "y_pred_ = knn.predict(X_test)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_))\n",
    "print()\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a515332",
   "metadata": {},
   "source": [
    "The knn score suggest there are somewhat tight relationships with the data.  However, the \"high\" mortality classification prediction is not as high suggesting mortality from COVID is not that correlated to the gdp per capita or the population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fa439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model complexity\n",
    "train_accuracies = {}\n",
    "test_accuracies = {}\n",
    "neighbors = np.arange(1,26) #(1,26)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through neighbors array\n",
    "for neighbor in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=neighbor)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracies[neighbor]=knn.score(X_train, y_train)\n",
    "    test_accuracies[neighbor]=knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1293b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot training and test values\n",
    "#plt.figure(figuresize=(8,6))\n",
    "plt.title(\"KNN: varying number of neighbors\")\n",
    "plt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n",
    "plt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Neighbors\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4eeaa",
   "metadata": {},
   "source": [
    "Tried with 5, 6, then 13 but this shows k of 3 is a good choice as this displays the highest testing accuracy and training score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9081bc1",
   "metadata": {},
   "source": [
    "<h4>Supervised learning with regression</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c11c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#training and testing the data\n",
    "#from datacamp\n",
    "\n",
    "#covid_data_ml\n",
    "#covid_data_ml1\n",
    "#covid_data_ml2 \n",
    "#print(ml_data)\n",
    "\n",
    "ml_data = covid_data_ml1\n",
    "\n",
    "\n",
    "X = ml_data.drop(\"total_deaths_per_100k\",axis=1).values #drop target value\n",
    "y = ml_data[\"total_deaths_per_100k\"].values #target observations\n",
    "\n",
    "\n",
    "# predicting mortality using population density\n",
    "\n",
    "#predict using pop_density (6)\n",
    "X_pop_d = X[:,6] \n",
    "#print(y.shape, X_pop_d.shape) # check shape\n",
    "# reshape\n",
    "X_pop_d = X_pop_d.reshape(-1,1) \n",
    "#print(X_pop_d.shape) #check shape\n",
    "\n",
    "#regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_pop_d,y)\n",
    "predictions = reg.predict(X_pop_d)\n",
    "print(predictions[:10])\n",
    "\n",
    "#plot Total_deaths per 100k vs. population density with regression\n",
    "plt.scatter(X_pop_d, y)\n",
    "plt.plot(X_pop_d, predictions, color = \"gray\")\n",
    "plt.ylabel(\"Mortality per 100k\")\n",
    "plt.xlabel(\"Population density\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355eb586",
   "metadata": {},
   "source": [
    "Weak negative correlation. The higher the population density the less likey the mortality from COVID, this is unexpected. I would expect the line to be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c0f30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predicting mortality using gdp\n",
    "\n",
    "#predict using gdp_per_capita (7)\n",
    "X_gdp_c = X[:,7] \n",
    "\n",
    "#print(ml_data)\n",
    "\n",
    "# reshape\n",
    "X_gdp_c = X_gdp_c.reshape(-1,1) \n",
    "#print(X_gdp_c.shape) #check shape\n",
    "\n",
    "#regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_gdp_c,y)\n",
    "predictions = reg.predict(X_gdp_c)\n",
    "print(predictions[:10])\n",
    "\n",
    "#plot Total_deaths per 100k vs. population density\n",
    "plt.scatter(X_gdp_c, y)\n",
    "plt.plot(X_gdp_c, predictions, color = \"gray\")\n",
    "plt.ylabel(\"Mortality per 100k\")\n",
    "plt.xlabel(\"GDP per Capita\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed9f9d5",
   "metadata": {},
   "source": [
    "Weak positive correlation. The higher the gdp per capita the less likey the mortality from COVID, this is somewhat expected, I would have expected the line to be steeper and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression using all features\n",
    "\n",
    "# need to drop mortality\n",
    "covid_data_sns.drop([\"date\",\"iso_code\",\"location\",\"total_cases\", \"total_deaths\",\"mortality\"],axis=1)\n",
    "\n",
    "\n",
    "ml_data_r = covid_data_sns.drop([\"date\",\"iso_code\",\"location\",\"total_cases\", \"total_deaths\",\"mortality\"],axis=1)\n",
    "\n",
    "ml_data_r\n",
    "\n",
    "\n",
    "\n",
    "X = ml_data_r.drop(\"total_deaths_per_100k\",axis=1).values #drop target value\n",
    "y = ml_data_r[\"total_deaths_per_100k\"].values #target observations\n",
    "\n",
    "\n",
    "#split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=21)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "#fit the linear regression to the training data\n",
    "reg_all = LinearRegression()\n",
    "reg_all.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#predict on the test set\n",
    "y_pred = reg_all.predict(X_test)\n",
    "\n",
    "r_score = reg_all.score(X_test, y_test)\n",
    "\n",
    "print(\"Predictions: {}, Actual Values: {}\".format(y_pred[:4], y_test[:4]))\n",
    "print(\"There are large gaps between the predictions and test data\")\n",
    "print(\"The model only explains about %5.2f\"%(r_score*100)+\"% of mortality level variance\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349027ac",
   "metadata": {},
   "source": [
    "<h6>Results summary</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc15a3",
   "metadata": {},
   "source": [
    "<pre style=\"font-family:arial;\">\n",
    "    Per the charts and analysis above, the results are not encoraging based on my inital hypothesis: that higher population density and lower GDP per capita for a country would have a negative impact on COVID mortality (higher deaths).  I beleive there may be some outliers, as seen in the scatter plot data, that should be reviewed further. This would potentially provide better results.\n",
    "    \n",
    "    I would have liked to had time to understand better the impact of the country specifc attributes on the data because my feeling is the scatter plot data does not appear to me to correlate to the result data.  Using these results it does not appear that we can reply strongly on GDP or population to forecast mortalitly.  I would be nice to continue this and test against other policies like masking and stay at home orders to see what an impact it had and if those policies impacted this disease.\n",
    "    \n",
    "    Overall, the data shows some correlations but fairly weak. the k score looked promising at .702 and the Classification report F1 score of 0.79/0.78 was ok performance. The confusion matrix results were ok (18 true positive and 15 for the false negative while 7 false positives compared to 48 true negative.  I think this may have been skewed by the fairly wide grouping I gave for \"high\" mortality vs \"low\". I should probably have left the \"mortality\" field as just \"high\" as it made understanding the confusion matirx more difficult, or confusing.\n",
    "    \n",
    "    Given the flatness of the regression line it would make sense to review some of the outlier data and rerun maybe with a wider set of data.\n",
    "    \n",
    "    I reviewed the hypertuning and I' not condifent it will imporve the performace of the report by much and given the lower accuracy I do not think over fitting is an issue.  For those reasons I chose not to use the hyper parameter tuning and focus on the machine learning module\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3e349",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#00008b;\">Insights</h2>\n",
    "(Point out at least 5 insights in bullet points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485bc359",
   "metadata": {},
   "source": [
    "* GDP and population density do not appear to be strongly correlated to mortality due to COVID.  I would like to have used the country data better the machine learning would probably give better insights into the correlation\n",
    "* The negative skew on the population density was quite surprising.  I expected the denser a population the greater the chance of COVID spearding\n",
    "* I really expected there to be a tighter correlation between the data and need more time to review the data for items that could be corrected\n",
    "* Some of the visuals can become overly complex and it takes time to simplify the visual into a basic form for interpretation\n",
    "* The amount of information to learn about python and the amount of insights is daunting and takes patience and perseverence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f7443",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#00008b;\">References</h2>\n",
    "HTML Code help: <a href=\"https://www.w3schools.com/html/html_links.asp\" target=\"_blank\">W3 Schools</a> \n",
    "\n",
    "Our World in Data (OWID): <a href=\"https://ourworldindata.org/coronavirus#explore-the-global-situation\" target=\"_blank\">https://ourworldindata.org/coronavirus#explore-the-global-situation</a> \n",
    "\n",
    "The World Bank GDP: <a href=\"https://data.worldbank.org/indicator/NY.GDP.MKTP.CD?year_high_desc=false\" target=\"_blank\">https://data.worldbank.org/indicator/NY.GDP.MKTP.CD?year_high_desc=false</a>\n",
    "\n",
    "Python:\n",
    "    <p>formatting numbers: https://pythonguides.com/python-format-number-with-commas/#:~:text=Python%20format%20number%20with%20commas%20Let%20us%20see,comma%20to%20every%20thousand%20places%20starting%20from%20left.</p>\n",
    "    <p>formatting dates: https://stackabuse.com/how-to-format-dates-in-python/</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543726da",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#00008b;\">Other Techniques Demonstrated</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206acabb",
   "metadata": {},
   "source": [
    "Merge DataFrames COVID_data to GDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid__data_merge1 = covid_data[covid_data[\"date\"].isin([last_date_n])]\n",
    "\n",
    "#rename location to Country_Name\n",
    "covid__data_merge1.rename(columns = {'location':'Country_Name'}, inplace = True)\n",
    "\n",
    "print(\"\\nAfter modifying column:\\n\", covid__data_merge1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717427b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gdp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GDP NaN values \n",
    "gdp_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc25112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with Nan values for GDP data\n",
    "#fill NaN with an average\n",
    "fill_value = pd.DataFrame({col: gdp_data.mean(axis=1) for col in gdp_data.columns})\n",
    "gdp_data.fillna(fill_value, inplace=True)\n",
    "\n",
    "\n",
    "gdp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01a35a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge Covid to Gdp\n",
    "df_covid_merge =pd.merge(covid__data_merge1,gdp_data, how=\"outer\")\n",
    "\n",
    "print(df_covid_merge.columns)\n",
    "df_covid_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regedit to find a strin of characters:\n",
    "\n",
    "# I did not use this code in the filtering above becuase tehre were other easier ways to find the list of OWID iso codes\n",
    "\n",
    "test = \"how am i to find OWID_ or owid or 'OWID' any other set of strings\"\n",
    "\n",
    "headers=covid_data_raw[\"iso_code\"].tolist()\n",
    "unique_headers = unique(headers)\n",
    "\n",
    "\n",
    "owid = re.findall(\"OWID\",str(headers))\n",
    "print(unique(owid))\n",
    "\n",
    "if owid:\n",
    "  print(\"I found a match\")\n",
    "else:\n",
    "  print(\"No match\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
